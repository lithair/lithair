# Clustering Overview

Lithair provides built-in clustering capabilities for horizontal scaling and high availability through leader-follower replication.

## Architecture

```text
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Clients      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Node 0      â”‚ â”‚   Node 1      â”‚ â”‚   Node 2      â”‚
    â”‚   (Leader)    â”‚ â”‚  (Follower)   â”‚ â”‚  (Follower)   â”‚
    â”‚   Port 8080   â”‚ â”‚   Port 8081   â”‚ â”‚   Port 8082   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚              â–²                   â–²
            â”‚     Log Replication              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Concepts

### Leader/Follower Model

- **Leader**: Handles all write operations, replicates to followers
- **Followers**: Receive replicated data, can serve read operations
- **Static assignment**: Node 0 is always the leader (no automatic election)

### Replication Flow

1. Client sends write request to leader
2. Leader appends to local consensus log
3. Leader replicates entry to followers via `/_raft/append`
4. Followers acknowledge receipt
5. Leader commits and responds to client

### Snapshot Resync

When a follower falls too far behind (marked as "desynced"):

1. Leader creates a snapshot of current state
2. Leader sends snapshot to follower via `/_raft/snapshot`
3. Follower installs snapshot and resumes normal replication

## Quick Start

### Enable Clustering

```rust
use lithair_core::app::LithairServerBuilder;

let server = LithairServerBuilder::new()
    .port(8080)
    .with_raft_cluster(
        0,                                          // node_id
        vec!["127.0.0.1:8081", "127.0.0.1:8082"],   // peers
        true,                                       // is_leader
    )
    .build()
    .await?;
```

### Cluster Configuration

| Node | Port | node_id | is_leader | Peers |
|------|------|---------|-----------|-------|
| Leader | 8080 | 0 | true | 8081, 8082 |
| Follower 1 | 8081 | 1 | false | 8080, 8082 |
| Follower 2 | 8082 | 2 | false | 8080, 8081 |

## Monitoring Endpoints

### Health Check

```bash
curl http://127.0.0.1:8080/_raft/health
```

Response:
```json
{
  "node_id": 0,
  "state": "Leader",
  "term": 1,
  "commit_index": 150,
  "last_applied": 150,
  "followers": {
    "127.0.0.1:8081": "healthy",
    "127.0.0.1:8082": "healthy"
  }
}
```

### Sync Status (Leader only)

```bash
curl http://127.0.0.1:8080/_raft/sync-status
```

Response:
```json
{
  "node_id": 0,
  "is_leader": true,
  "commit_index": 150,
  "followers": [
    {
      "address": "127.0.0.1:8081",
      "health": "healthy",
      "last_replicated_index": 150,
      "lag": 0,
      "last_latency_ms": 2,
      "consecutive_failures": 0
    },
    {
      "address": "127.0.0.1:8082",
      "health": "healthy",
      "last_replicated_index": 150,
      "lag": 0,
      "last_latency_ms": 1,
      "consecutive_failures": 0
    }
  ]
}
```

### Resync Statistics

```bash
curl http://127.0.0.1:8080/_raft/resync_stats
```

Response:
```json
{
  "node_id": 0,
  "is_leader": true,
  "resync_stats": {
    "snapshots_created": 2,
    "send_attempts": 3,
    "send_successes": 2,
    "send_failures": 1
  }
}
```

## Ops Manual Intervention

### Force Resync a Follower

When a follower is stuck in "desynced" state or needs manual recovery:

```bash
# Check current sync status
curl http://127.0.0.1:8080/_raft/sync-status

# Force snapshot resync to specific follower
curl -X POST "http://127.0.0.1:8080/_raft/force-resync?target=127.0.0.1:8082"
```

Response:
```json
{
  "target": "127.0.0.1:8082",
  "success": true,
  "message": "Snapshot resync to 127.0.0.1:8082 completed successfully"
}
```

### Node Recovery Procedure

1. **Check cluster status**: Identify which nodes are down or desynced
2. **Restart failed node**: Node will auto-resync if data is recoverable
3. **Force resync if needed**: Use `/_raft/force-resync` endpoint
4. **Verify consistency**: Check that all nodes have same data count

## Follower Health States

| State | Icon | Meaning | Action |
|-------|------|---------|--------|
| healthy | ğŸŸ¢ | In sync, replication working | None |
| lagging | ğŸŸ¡ | Behind but catching up | Monitor |
| desynced | ğŸ”´ | Too far behind, needs snapshot | Auto/manual resync |
| unknown | âšª | No replication activity yet | Wait or investigate |

## Configuration Options

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `LITHAIR_NODE_ID` | 0 | Node identifier |
| `LITHAIR_RAFT_PEERS` | - | Comma-separated peer addresses |
| `LITHAIR_IS_LEADER` | false | Whether this node is the leader |

### Replication Tuning

| Parameter | Default | Description |
|-----------|---------|-------------|
| Batch size | 100 | Max entries per replication batch |
| Flush interval | 5ms | WAL group commit interval |
| Resync threshold | 1000 | Entries behind before snapshot resync |
| Snapshot timeout | 60s | Max time for snapshot transfer |

## Limitations

| Feature | Status | Notes |
|---------|--------|-------|
| Leader election | Static | Node 0 is always leader |
| Quorum writes | Best-effort | Async replication |
| Split-brain protection | Not implemented | Assumes single leader |
| TLS inter-node | Not implemented | Use network isolation |
| Network partitions | Basic | Followers marked desynced |

## Future Roadmap

- [ ] Automatic leader election
- [ ] Strict quorum writes (majority acknowledgment)
- [ ] TLS for inter-node communication
- [ ] Dynamic cluster membership changes
- [ ] Read scaling (consistent reads from followers)

## See Also

- [Stress Testing Example](../../../examples/replication_stress_test/README.md)
- [Persistence Overview](../persistence/overview.md)
- [Event Sourcing](../persistence/event-store.md)
