# ğŸš€ Lithair Distributed Replication Demo

Demo d'un cluster Lithair multi-nÅ“uds avec rÃ©plication automatique des donnÃ©es.

## ğŸ¯ Objectif

Cet exemple montre comment :
- Configurer un cluster Lithair distribuÃ©
- RÃ©pliquer automatiquement les donnÃ©es entre nÅ“uds
- Utiliser le modÃ¨le dÃ©claratif avec attributs de persistance
- GÃ©rer la redirection vers le leader et la rÃ©plication HTTP (OpenRaft complet: WIP)

## ğŸ—ï¸ Architecture

```
Node 1 (Leader)     Node 2 (Follower)    Node 3 (Follower)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Port: 8080   â”‚â—„â”€â”€â”€â”¤ Port: 8081     â”‚    â”‚ Port: 8082     â”‚
â”‚ Data: node1  â”‚    â”‚ Data: node2    â”‚â—„â”€â”€â”€â”¤ Data: node3    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                    â–²                       â–²
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         Raft Consensus
```

## ğŸ“‹ FonctionnalitÃ©s

### ModÃ¨le DÃ©claratif avec RÃ©plication
- **Product**: ModÃ¨le produit avec clÃ© primaire, champs auditÃ©s et rÃ©plication
- **Attributs de persistance**: `#[persistence(replicate, track_history)]`

### Ã‰vÃ©nements DistribuÃ©s
- CrÃ©ation/modification d'utilisateurs
- CrÃ©ation/modification de messages
- Statistiques de rÃ©plication par nÅ“ud

## ğŸš€ Usage

### DÃ©marrage du Cluster

```bash
# Terminal 1: Lancer le leader (Node 1)
cargo run --release --bin replication-declarative-node -- \
  --node-id 1 \
  --port 8080 \
  --peers "8081,8082"

# Terminal 2: Lancer le follower (Node 2)
cargo run --release --bin replication-declarative-node -- \
  --node-id 2 \
  --port 8081 \
  --peers "8080,8082"

# Terminal 3: Lancer le follower (Node 3)
cargo run --release --bin replication-declarative-node -- \
  --node-id 3 \
  --port 8082 \
  --peers "8080,8081"
```

### Monitorer la RÃ©plication

Chaque nÅ“ud affiche ses statistiques toutes les 10 secondes :
```
=== Node 1 Statistics ===
Users: 2 local, 6 total
Messages: 3 local, 9 total  
Replications: 6 received, 12 sent
==============================
```

## ğŸ”§ Configuration

### Attributs DÃ©claratifs de Persistance (extrait simplifiÃ©)

```rust
#[derive(DeclarativeModel)]
pub struct Product {
    #[db(primary_key, indexed)]
    #[lifecycle(immutable)]
    #[http(expose)]
    #[persistence(replicate, track_history)]
    pub id: Uuid,

    #[db(indexed, unique)]
    #[lifecycle(audited, retention = 90)]
    #[http(expose, validate = "non_empty")]
    #[persistence(replicate, track_history)]
    pub name: String,
}
```

### Options Disponibles
- `replicate`: RÃ©plique sur tous les nÅ“uds du cluster
- `track_history`: Conserve l'historique complet des modifications
- `memory_only`: DonnÃ©es locales uniquement (pas de persistance/rÃ©plication)
- `auto_persist`: Persistance automatique des Ã©critures
- `no_replication`: Exclut de la rÃ©plication mÃªme si persistÃ©

## ğŸ“Š Monitoring

### MÃ©triques par NÅ“ud
- **users_created**: Utilisateurs crÃ©Ã©s localement
- **messages_created**: Messages crÃ©Ã©s localement  
- **replications_received**: Ã‰vÃ©nements reÃ§us d'autres nÅ“uds
- **replications_sent**: Ã‰vÃ©nements envoyÃ©s aux autres nÅ“uds

### Persistence
- Ã‰vÃ©nements persistÃ©s dans un EventStore local par nÅ“ud (fichiers `.raftlog`)
- Snapshots pÃ©riodiques pour accÃ©lÃ©rer la reprise (si activÃ©s)

## ğŸ§ª Tests de RÃ©plication

### ScÃ©narios TestÃ©s
1. **CrÃ©ation distribuÃ©e**: Chaque nÅ“ud crÃ©e des utilisateurs/messages
2. **Contraintes uniques**: VÃ©rification des doublons cross-nÅ“uds
3. **ClÃ©s Ã©trangÃ¨res**: CohÃ©rence des relations entre entitÃ©s
4. **RÃ©cupÃ©ration**: RedÃ©marrage de nÅ“uds et rattrapage

### Ordre d'ExÃ©cution des Tests
1. DÃ©marrer tous les nÅ“uds
2. Attendre la formation du cluster
3. ExÃ©cuter les opÃ©rations en parallÃ¨le sur chaque nÅ“ud
4. VÃ©rifier la cohÃ©rence des donnÃ©es rÃ©pliquÃ©es

## ğŸ”® Prochaines Ã‰tapes (TODO)

- [ ] IntÃ©gration OpenRaft complÃ¨te (consensus fort)
- [ ] Gestion des partitions rÃ©seau
- [ ] Tests de performance sous charge Ã©levÃ©e
- [ ] Interface web de monitoring du cluster

## ğŸ›ï¸ Arguments de Ligne de Commande

```bash
--node-id <ID>              # ID unique du nÅ“ud (obligatoire)
--port <PORT>               # Port d'Ã©coute (dÃ©faut: 8080)
--peers "<PORT1>,<PORT2>"   # Autres nÅ“uds: ports des pairs sur localhost
```

## ğŸ’¡ Notes d'ImplÃ©mentation

- Serveur HTTP basÃ© sur Hyper (HTTP/1.1)
- Redirection automatique des Ã©critures vers le leader
- RÃ©plication des donnÃ©es via HTTP entre nÅ“uds
- Ã‰vÃ©nements sÃ©rialisÃ©s en JSON pour le transport rÃ©seau

## ğŸ§ª Benchmarks

Un script est fourni pour lancer un benchmark CRUD distribuÃ©:

```bash
./bench_1000_crud_parallel.sh 1000
```

Consultez `baseline_results/` Ã  la racine du repo pour des mesures reprÃ©sentatives.

## ğŸ” HTTP Hardening Demo (stateless perf + firewall)

Le binaire `replication-hardening-node` lance un serveur HTTP dÃ©claratif minimal pour dÃ©montrerÂ :

- Endpoints de performance sans Ã©tat (`/perf/echo`, `/perf/json`, `/perf/bytes`)
- Gzip (nÃ©gociation `Accept-Encoding`, seuil configurable)
- Politiques par prÃ©fixe (exÂ : forcer gzip / `no-store` sur `/perf`)
- Firewall (allow/deny IP, CIDR, macros `internal`, `loopback`, etc.)

Par dÃ©faut, ce serveur dÃ©marre avec une posture Â«Â production-likeÂ Â»Â :

- `/perf/*` et `/metrics` protÃ©gÃ©s par firewall
- `/status` et `/health` exemptÃ©s
- `allow` inclut la macro `internal` (rÃ©seaux privÃ©s IPv4 + ULA IPv6)

Pour lâ€™ouvrir en local (dÃ©sactiver la posture firewall par dÃ©faut)Â :

```bash
cargo run -p replication --bin replication-hardening-node -- --port 18320 --open
```

Vous pouvez aussi compiler lâ€™exemple en mode Â«Â ouvert par dÃ©fautÂ Â» via la featureÂ :

```bash
cargo run -p replication --features open_by_default --bin replication-hardening-node -- --port 18320
```

Le script de bench stateless lance automatiquement le serveur avec `--open`Â :

```bash
bash examples/09-replication/bench_http_server_stateless.sh
```

### Mode Single-Node (Isolation du moteur/persistance)

Pour isoler lâ€™overhead rÃ©seau/consensus et mesurer uniquement le coÃ»t HTTP + moteur + persistance, vous pouvez lancer le benchmark en **monoâ€‘nÅ“ud**Â :

```bash
SINGLE_NODE=1 ./bench_1000_crud_parallel.sh 10000
```

AstuceÂ : combinez avec les variables `LT_` pour comparer JSON vs Binaire, async on/offÂ :

```bash
# Async JSON (Stage A)
SINGLE_NODE=1 LT_OPT_PERSIST=1 LT_ENABLE_BINARY=0 ./bench_1000_crud_parallel.sh 10000

# Binaire (Stage B)
SINGLE_NODE=1 LT_OPT_PERSIST=1 LT_ENABLE_BINARY=1 ./bench_1000_crud_parallel.sh 10000
```

## âš™ï¸ Runtime (Persistence & Performance)

Pour des benchmarks rÃ©alistes Ã  haut dÃ©bit, le demo supporte des variables dâ€™environnement `LT_` qui pilotent la persistance de lâ€™EventStoreÂ :

- `LT_OPT_PERSIST` (1/0) â€“ active lâ€™Ã©criture asynchrone optimisÃ©e (writer thread) pour les Ã©vÃ©nements (par dÃ©faut activÃ©e dans le script de bench).
- `LT_BUFFER_SIZE` (octets) â€“ taille du buffer dâ€™Ã©criture (par dÃ©faut 1â€¯048â€¯576 = 1â€¯Mo).
- `LT_MAX_EVENTS_BUFFER` â€“ nombre dâ€™Ã©vÃ©nements Ã  mettre en tampon avant flush (par dÃ©faut 2000).
- `LT_FLUSH_INTERVAL_MS` â€“ intervalle de flush pÃ©riodique (par dÃ©faut 5â€¯ms pour les benchs).
- `LT_FSYNC_ON_APPEND` (1/0) â€“ fsync Ã  chaque append (0 recommandÃ© pour les benchs de dÃ©bit).
- `LT_EVENT_MAX_BATCH` â€“ taille de lot (batch) interne cÃ´tÃ© EventStore (par dÃ©faut 65536 dans le script de bench).
- `LT_ENABLE_BINARY` (1/0) â€“ active le mode binaire (Stageâ€¯B)â€¯: les enveloppes dâ€™Ã©vÃ©nements sont sÃ©rialisÃ©es en bincode et Ã©crites lignes par lignes (sÃ©parÃ©es par `\n`). Rejouer/restaurer reste compatibleâ€¯: le moteur reconvertit en JSON lors de la lecture.
- `LT_DISABLE_INDEX` (1/0) â€“ dÃ©sactive lâ€™index `aggregate_id -> offset` pour Ã©viter des Ã©critures supplÃ©mentaires pendant les benchs.
- `LT_DEDUP_PERSIST` (1/0) â€“ contrÃ´le la persistance des IDs dâ€™idempotence. Mettre Ã  `0` pour les benchs Ã©phÃ©mÃ¨res (pas dâ€™exactlyâ€‘once crossâ€‘restart nÃ©cessaire).

Exemple dâ€™exÃ©cution manuelle avec persistance optimisÃ©e et binaireÂ :

```bash
export LT_OPT_PERSIST=1
export LT_BUFFER_SIZE=1048576
export LT_MAX_EVENTS_BUFFER=5000
export LT_FLUSH_INTERVAL_MS=2
export LT_FSYNC_ON_APPEND=0
export LT_ENABLE_BINARY=1

./bench_1000_crud_parallel.sh 10000
```

NotesÂ :

- Le script `bench_1000_crud_parallel.sh` exporte dÃ©jÃ  des valeurs par dÃ©faut adaptÃ©es pour le dÃ©bit, dont `LT_OPT_PERSIST=1`.
- Le mode binaire (`LT_ENABLE_BINARY=1`) maximise la vitesse dâ€™append (3â€“5Ã— vs JSON selon les charges) tout en conservant des snapshots JSON.

### Profils de stockage prÃ©dÃ©finis (STORAGE_PROFILE)

Le script de bench supporte des profils prÃªts Ã  lâ€™emploi (sÃ©lection via `STORAGE_PROFILE=<nom>`):

- `high_throughput` (par dÃ©faut)
  - ObjectifÂ : DÃ©bit maximum (benchmarks). Async writer ON, binaire ON, index/dedup OFF, gros buffers, fsync OFF, snapshots trÃ¨s espacÃ©s.
  - ExempleÂ :
    ```bash
    STORAGE_PROFILE=high_throughput LOADGEN_MODE=bulk LOADGEN_BULK_SIZE=500 \
    ./bench_1000_crud_parallel.sh 10000
    ```

- `balanced`
  - ObjectifÂ : Compromis dÃ©bit/fiabilitÃ©. Async ON, binaire ON, index/dedup ON, buffers moyens, fsync OFF.
  - ExempleÂ :
    ```bash
    STORAGE_PROFILE=balanced LOADGEN_MODE=bulk LOADGEN_BULK_SIZE=500 \
    ./bench_1000_crud_parallel.sh 10000
    ```

- `durable_security`
  - ObjectifÂ : DurabilitÃ© et audit trail. Async ON, binaire OFF (lisibilitÃ©), index/dedup ON, fsync ON, snapshots frÃ©quents.
  - ExempleÂ :
    ```bash
    STORAGE_PROFILE=durable_security LOADGEN_MODE=bulk LOADGEN_BULK_SIZE=200 \
    ./bench_1000_crud_parallel.sh 10000
    ```

Chaque profil configure automatiquement les variables `LT_` adÃ©quates (buffers, flush, fsync, index, dedup, snapshots) afin dâ€™adapter le moteur aux besoins de lâ€™application.

### Chemin de donnÃ©es (EXPERIMENT_DATA_BASE)

Par dÃ©faut, le script de bench configure la base de donnÃ©es de lâ€™exemple dans:

```
EXPERIMENT_DATA_BASE=examples/09-replication/data
```

Ce chemin est transmis au moteur via la variable dâ€™environnement `EXPERIMENT_DATA_BASE` et remplace `EngineConfig.event_log_path` au dÃ©marrage. Vous pouvez donc:

- Laisser le comportement par dÃ©faut (les fichiers `.raftlog`/snapshots sont Ã©crits dans le dossier de lâ€™exemple)
- Ou bien surcharger le chemin:

```bash
EXPERIMENT_DATA_BASE=/tmp/lithair_bench \
STORAGE_PROFILE=high_throughput LOADGEN_MODE=bulk LOADGEN_BULK_SIZE=1000 \
./bench_1000_crud_parallel.sh 100000
```

Le script affiche explicitement le chemin utilisÃ© et liste les fichiers persistÃ©s en fin de run.

## ğŸ”¦ Lectures lÃ©gÃ¨res (LIGHT_READS)

Pour Ã©viter le coÃ»t de sÃ©rialisation JSON de la liste complÃ¨te (`GET /api/products`), le bench supporte des lectures Â« lÃ©gÃ¨res Â» configurables via `LIGHT_READS`Â :

- `LIGHT_READS=0` (dÃ©faut) â†’ `GET /api/products` (liste complÃ¨te, lecture lourde)
- `LIGHT_READS=1`, `true` ou `status` â†’ `GET /status` (trÃ¨s lÃ©ger)
- `LIGHT_READS=count` â†’ `GET /api/products/count` (lÃ©ger, retourne `{ "count": N }`)

Endpoints ajoutÃ©s par le serveur dÃ©claratif (`lithair-core/src/http/declarative.rs`)Â :

- `GET /api/{model}/count` â†’ renvoie uniquement le nombre dâ€™Ã©lÃ©ments
- `GET /api/{model}/random-id` â†’ renvoie un `id` existant (utile pour prÃ©remplir les UPDATE sans lister tout)

### A/B test Â« heavy vs light Â»

Exemple aprÃ¨s prÃ©-seed (5â€¯000 objets par nÅ“ud)Â :

```bash
# Heavy read: liste complÃ¨te
LIGHT_READS=0 PRESEED_PER_NODE=5000 CREATE_PERCENTAGE=0 READ_PERCENTAGE=100 UPDATE_PERCENTAGE=0 \
  ./bench_1000_crud_parallel.sh 3000

# Light read: compteur
LIGHT_READS=count PRESEED_PER_NODE=5000 CREATE_PERCENTAGE=0 READ_PERCENTAGE=100 UPDATE_PERCENTAGE=0 \
  ./bench_1000_crud_parallel.sh 3000
```

Dans nos mesures rÃ©centesÂ :

Observations rÃ©centes (3 nÅ“uds, PRESEED_PER_NODE=50000, concurrency=256, lecture seule 3000 ops)Â :

- Heavy read (liste complÃ¨te) â‰ˆ 38.6 ops/s, p50 â‰ˆ 6.1 s, p95 â‰ˆ 10 s
- Light read (count) â‰ˆ 10.3kâ€“15.3k ops/s, p50 â‰ˆ 2â€“3 ms, p95 â‰ˆ 115â€“128 ms
- Status â‰ˆ 15.1kâ€“24.6k ops/s, p50 â‰ˆ 1â€“2 ms, p95 â‰ˆ 80â€“170 ms

RecommandationsÂ :
- Ã‰vitez `GET /api/products` pour les benchmarks de perf; utilisez `/count` ou `/status`.
- Profil `high_throughput`Â : par dÃ©faut `LOADGEN_CONCURRENCY=256` offre le meilleur compromis dÃ©bit/tails.
- Profils `balanced` et `durable_security`Â : rester â‰¤512 pour contenir les tails dâ€™Ã©criture.
- La suite `BENCH_SUITE=durability_profiles` redÃ©marre le cluster Ã  chaque profil afin dâ€™appliquer correctement les paramÃ¨tres de stockage.

AstuceÂ : pour des workloads Ã  forte proportion dâ€™UPDATE, le loadgen utilise dÃ©sormais `GET /api/products/random-id` pour rÃ©cupÃ©rer un `id` lÃ©ger si la pool dâ€™ID est vide (pas de `GET /api/products`).